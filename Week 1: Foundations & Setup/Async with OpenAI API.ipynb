{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a31dec0-0bbd-4e0c-a45d-77364146a7eb",
   "metadata": {},
   "source": [
    "# Async with OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c112a-9da6-4458-9ef8-2d8d5fe0d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "client = AsyncOpenAI(api_key=\"your-api-key\")\n",
    "\n",
    "async def ask(prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def main():\n",
    "    responses = await asyncio.gather(\n",
    "        ask(\"Explain quantum computing\"),\n",
    "        ask(\"Summarize Moby Dick\"),\n",
    "    )\n",
    "    for res in responses:\n",
    "        print(res)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70919ab9-2dc4-487e-82d9-caea72e654ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d93ec36c-11da-44e2-bbf7-04a265c1d6b9",
   "metadata": {},
   "source": [
    "# Full Async LLM Script Using AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ffc50-16a9-456d-ac72-b4d614fea94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In terminal:\n",
    "\n",
    "export OPENAI_API_KEY=\"your-api-key-here\"  # macOS/Linux\n",
    "# or\n",
    "set OPENAI_API_KEY=\"your-api-key-here\"     # Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e6740-8c37-4356-9d00-4c31ba336936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Optional: Set API key here, or use env variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def call_llm(prompt: str) -> str:\n",
    "    print(f\"Sending prompt: {prompt}\")\n",
    "    \n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # Or \"gpt-3.5-turbo\"\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    print(f\"Received response for: {prompt}\")\n",
    "    return answer\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"Explain LangChain in simple terms.\",\n",
    "        \"List 3 benefits of using async in Python.\",\n",
    "        \"What are the use-cases of vector databases with LLMs?\"\n",
    "    ]\n",
    "\n",
    "    tasks = [call_llm(p) for p in prompts]\n",
    "\n",
    "    print(\"\\nSending prompts asynchronously...\\n\")\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    print(\"\\nResponses:\\n\")\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"\\nPrompt {i}:\\n{res}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "\n",
    "async def wrapper():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(main())\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "await wrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25586c51-b640-4795-8a6e-56d08e707426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f7a578-c5f2-40c7-93eb-0fde562ea26f",
   "metadata": {},
   "source": [
    "# Limiting Concurrent API Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5f001b-9888-408b-a438-8fdc3e091669",
   "metadata": {},
   "source": [
    "## ü§î What Does \"Limiting Concurrent API Calls\" Mean?\n",
    "\n",
    "When you use asyncio.gather() to call multiple APIs, they all run at the same time (concurrently).\n",
    "\n",
    "But sometimes:\n",
    "\n",
    "OpenAI or other providers rate-limit your requests (e.g., 10 requests per second).\n",
    "\n",
    "Your app might overwhelm the API or your local resources (network, CPU).\n",
    "\n",
    "You want to control load, especially in production.\n",
    "\n",
    "### üîê Limiting concurrency means:\n",
    "‚û°Ô∏è ‚ÄúOnly allow X number of API calls to happen at once, even if I have 100 total tasks.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e72fc-d66a-40c5-b175-6b637b89c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "async def call_llm(prompt: str, semaphore: asyncio.Semaphore) -> str:\n",
    "    async with semaphore:\n",
    "        print(f\"Sending: {prompt}\")\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"What's the capital of France?\",\n",
    "        \"Explain quantum computing.\",\n",
    "        \"How does LangChain work?\",\n",
    "        \"Summarize the Matrix movie.\",\n",
    "        \"List 3 benefits of Python.\"\n",
    "    ]\n",
    "\n",
    "    semaphore = asyncio.Semaphore(2)  # Only 2 requests at once\n",
    "\n",
    "    tasks = [\n",
    "        call_llm(prompt, semaphore)\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"\\nPrompt {i}:\\n{res}\")\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "\n",
    "async def wrapper():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(main())\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "await wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c6bbf7-ed04-4842-a2f7-ccbbb9eedd08",
   "metadata": {},
   "source": [
    "## Why This Matters in LLM Projects\n",
    "\n",
    "In large applications (like LangChain chains with multiple tools or prompts):\n",
    "* You often trigger many API calls in parallel.\n",
    "* OpenAI may throttle or reject your calls.\n",
    "* This pattern keeps your app polite and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01caaac-52be-4539-b0a4-13ebe54093c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b70435b-ae12-4c41-abab-65d813af0cc5",
   "metadata": {},
   "source": [
    "# Add error handling for rate limits and timeouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d1738-742f-4c78-895e-daba73c43d4c",
   "metadata": {},
   "source": [
    "### Final Working Version (Error Handling + Retry + Concurrency Limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85216d95-0df9-4c06-8779-5d9e87472f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from openai import AsyncOpenAI, RateLimitError, APITimeoutError, APIConnectionError\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "\n",
    "\n",
    "# Create the client and a semaphore\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "semaphore = asyncio.Semaphore(2)\n",
    "\n",
    "\n",
    "# Wrap the API call with @retry logic\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((RateLimitError, APIConnectionError, APITimeoutError)),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    stop=stop_after_attempt(3),\n",
    "    reraise=True\n",
    ")\n",
    "async def safe_call_llm(prompt: str) -> str:\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        timeout=30\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Add the semaphore and full error handling\n",
    "async def call_llm(prompt: str, semaphore: asyncio.Semaphore) -> str:\n",
    "    async with semaphore:\n",
    "        print(f\"Calling LLM for: {prompt}\")\n",
    "        try:\n",
    "            response = await safe_call_llm(prompt)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to handle prompt '{prompt}': {type(e).__name__} - {e}\")\n",
    "            return \"Error: Could not generate response.\"\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"What's the capital of France?\",\n",
    "        \"Explain quantum computing.\",\n",
    "        \"How does LangChain work?\",\n",
    "        \"Summarize the Matrix movie.\",\n",
    "        \"List 3 benefits of Python.\"\n",
    "    ]\n",
    "\n",
    "    tasks = [call_llm(p, semaphore) for p in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"\\nPrompt {i}:\\n{res}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.run(main())\n",
    "\n",
    "\n",
    "async def wrapper():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    try:\n",
    "        loop.run_until_complete(main())\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "await wrapper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415df6c-0235-4dbd-9dd8-579bb33615f8",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "| Technique                         | Why It's Important                        |\n",
    "| --------------------------------- | ----------------------------------------- |\n",
    "| `@retry` with `tenacity`          | Automatically retries on temporary errors |\n",
    "| `async with semaphore`            | Limits how many requests are sent at once |\n",
    "| `asyncio.gather()`                | Runs many LLM calls concurrently          |\n",
    "| Exception handling (`try/except`) | Prevents app crashes on failure           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36615e82-458f-45b9-9f02-4e43b80893f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66c8a5ae-63c5-4715-a765-32d9752335a5",
   "metadata": {},
   "source": [
    "# LangChain agent\n",
    "\n",
    "Create a LangChain-based LLM agent or chain that:\n",
    "\n",
    "* Sends multiple prompts or tool calls concurrently\n",
    "\n",
    "* Limits concurrency using asyncio.Semaphore\n",
    "\n",
    "* Retries failed calls with tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c823ca9-8203-43c8-840e-77b3b9117062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import tool\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from openai import RateLimitError, APIConnectionError\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "@tool\n",
    "def get_temperature(city: str) -> str:\n",
    "    import random\n",
    "    return f\"The temperature in {city} is {random.randint(15, 30)}¬∞C\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "tools = [get_temperature]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "semaphore = asyncio.Semaphore(2)  # Max 2 concurrent agent calls\n",
    "\n",
    "@retry(\n",
    "    retry=retry_if_exception_type((RateLimitError, APIConnectionError)),\n",
    "    wait=wait_exponential(multiplier=1, min=2, max=10),\n",
    "    stop=stop_after_attempt(3),\n",
    "    reraise=True\n",
    ")\n",
    "async def safe_call_chain(chain, input_text):\n",
    "    async with semaphore:\n",
    "        return await chain.ainvoke({\"input\": input_text})\n",
    "\n",
    "async def main():\n",
    "    prompts = [\n",
    "        \"What's the temperature in Berlin?\",\n",
    "        \"What's the temperature in Tokyo?\",\n",
    "        \"What's the temperature in San Francisco?\",\n",
    "        \"What's the temperature in Delhi?\"\n",
    "    ]\n",
    "\n",
    "    tasks = [safe_call_chain(agent, prompt) for prompt in prompts]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    for i, res in enumerate(results, 1):\n",
    "        print(f\"\\nPrompt {i} result:\\n{res['output']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140078b-d961-46a8-b48d-3a0d0c000085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f345060-5b35-470b-96de-5078c191c8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379e85f8-d0ba-41a6-a40c-61b1414614c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c8c97-36ed-4dba-bef4-531d67ef5c40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988e0f2-b3c5-4e1f-9aa1-770ee2bce6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
